---
title: "Challenge 3"
author: "Shuqi Hong"
description: "Tidy Data: Pivoting"
date: "6/07/2023"
format:
  html:
    toc: true
    code-fold: true
    code-copy: true
    code-tools: true
categories:
  - challenge_3
  - animal_weights
  - eggs
  - australian_marriage
  - usa_households
  - sce_labor
---

```{r}
#| label: setup
#| warning: false
#| message: false

library(tidyverse)

knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)

eggs <- read.csv("_data/eggs_tidy.csv")

head(eggs)
```
From the original file, it looks like the file is about the number of purchase of different sizes eggs in each year and months.

We can look the trend of the number in years. So first we drop the month colunm and sum all the number by years. The tibble is wide and not clear to see the year trend, so we pivot it.
```{r}
yeartrend <- eggs %>% select(-month) %>%group_by(year) %>%
  summarise_all(sum) %>%
  t()
yeartrend

eggs %>% select(-month) %>%group_by(year) %>%
  pivot_wider(names_from = year, values_from = c(large_half_dozen,large_dozen))



```
I expect that the colunm is each year and row is each dozen, which is like the dataframe after using t(). The pivot function seems like unvailable in this situation.

```{r}

eggs %>% group_by(year)  %>% pivot_longer(`large_half_dozen`: `extra_large_dozen`, names_to = "dozen", values_to = "number" ) %>% group_by(dozen,year) %>% 
  summarise( mean = mean(`number`)) %>% 
  pivot_wider(names_from = year, values_from = mean)

```
From this tibble, we can see the year trend of each dozen much clearer. We can also know people bought more extra large dozen and large dozen than other two scales in general.

```{r}

mean_January <- eggs %>% filter(month == "January") %>% 
  pivot_longer(`large_half_dozen`: `extra_large_dozen`, names_to = "January_dozen", values_to = "number" ) %>%
  group_by(January_dozen,year) %>%
  summarise( January_mean = mean(`number`)) %>% 
  pivot_wider(names_from = year, values_from = January_mean)
  
mean_January
  
```
We can also choose a specific month like January to see. The average value of extra_large_half_dozen in January from 2009 to 2012 didn't change which means it was quite steady. All scales of dozen had a large increase between 2008 and 2009. It should happen something in January 2009. Even though the general trend is increasing, the large_dozen has a little decrease from 2009.






### Example: find current and future data dimensions

One easy way to do this is to think about the dimensions of your current data (tibble, dataframe, or matrix), and then calculate what the dimensions of the pivoted data should be.

Suppose you have a dataset with $n$ rows and $k$ variables. In our example, 3 of the variables are used to identify a case, so you will be pivoting $k-3$ variables into a longer format where the $k-3$ variable names will move into the `names_to` variable and the current values in each of those columns will move into the `values_to` variable. Therefore, we would expect $n * (k-3)$ rows in the pivoted dataframe!

Lets see if this works with a simple example.

```{r}
#| tbl-cap: Example

df<-tibble(country = rep(c("Mexico", "USA", "France"),2),
           year = rep(c(1980,1990), 3), 
           trade = rep(c("NAFTA", "NAFTA", "EU"),2),
           outgoing = rnorm(6, mean=1000, sd=500),
           incoming = rlogis(6, location=1000, 
                             scale = 400))
df

#existing rows/cases
nrow(df)

#existing columns/cases
ncol(df)

#expected rows/cases
nrow(df) * (ncol(df)-3)

# expected columns 
3 + 2
```

Or simple example has $n = 6$ rows and $k - 3 = 2$ variables being pivoted, so we expect a new dataframe to have $n * 2 = 12$ rows x $3 + 2 = 5$ columns.


## Pivot the Data

Now we will pivot the data, and compare our pivoted data dimensions to the dimensions calculated above as a "sanity" check.

### Example

```{r}
#| tbl-cap: Pivoted Example

df<-pivot_longer(df, col = c(outgoing, incoming),
                 names_to="trade_direction",
                 values_to = "trade_value")
df
```

Yes, once it is pivoted long, our resulting data are $12x5$ - exactly what we expected!
